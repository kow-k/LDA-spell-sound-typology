{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890ca5f3",
   "metadata": {},
   "source": [
    "様々な言語の Spell と Sound (IPA) の構造を LDA で探索する\n",
    "黒田　航　(kow.kuroda@gmail.com)\n",
    "\n",
    "2024/03/26 i. FastText のエンコードを追加; ii. UMAP を使った次元圧縮を追加; iii. DBSCANクラスターに帰属する個数の相関をHeatmap にする処理を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe13212",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import sys, os, random, re, glob\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "#from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ad140",
   "metadata": {},
   "source": [
    "設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf1d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 処理設定\n",
    "verbose = False\n",
    "\n",
    "## 全言語をカバーできる色パレットの用意\n",
    "import plotly\n",
    "color_palette = plotly.colors.qualitative.Dark24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LDA 用 (FastText と併用可能)\n",
    "use_LDA = True\n",
    "\n",
    "## トピック数\n",
    "n_topics      = 15 # 30は多過ぎる？\n",
    "\n",
    "### DTM 構築: doc, term の設定\n",
    "## doc\n",
    "doc_type      = 'form'  # 変更不可\n",
    "doc_attrs     = [ 'spell', 'sound' ]\n",
    "doc_attr      = doc_attrs[1]\n",
    "print(f\"doc_attr: {doc_attr}\")\n",
    "\n",
    "## doc の長さの上限\n",
    "max_doc_size  = 15\n",
    "print(f\"max_doc_size: {max_doc_size}\")\n",
    "\n",
    "## doc の長さの下限\n",
    "min_doc_size  = 3\n",
    "print(f\"min_doc_size: {min_doc_size}\")\n",
    "\n",
    "## ngram を包括的にするかどうか\n",
    "ngram_is_inclusive = True\n",
    "\n",
    "## term\n",
    "term_size         = 'character' # 出力用の名目変数\n",
    "#term_types        = re.split(r\",\\s+\", \"1gram, 2gram, 3gram, skippy2gram, skippy3gram\")\n",
    "term_types        = [ '1gram', '2gram', '3gram', 'skippy2gram', 'skippy3gram' ]\n",
    "lda_term_type     = term_types[-2]\n",
    "print(f\"lda_term_type: {lda_term_type}\")\n",
    "\n",
    "## term の最低頻度\n",
    "term_min_freq = 1\n",
    "\n",
    "## 高頻度 term の濫用指標: 大きくし過ぎないように．terms の異なりが小さいので0.03 は十分に大きい\n",
    "term_abuse_threshold = 0.003\n",
    "\n",
    "## skippy n-gram の表記\n",
    "gap_mark = \"…\"\n",
    "\n",
    "## skippy n-gram の結合範囲\n",
    "max_ga_val = round(max_doc_size * 0.8)\n",
    "print(f\"max_ga_val for skippy n-grams: {max_ga_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastText を使うか (LDA と併用可能)\n",
    "use_FastText = True\n",
    "\n",
    "## term\n",
    "ft_term_types = [ '1gram', '2gram', '3gram', 'skippy2grm', 'skippy3gram' ]\n",
    "ft_term_type = ft_term_types[2]\n",
    "print(f\"ft_term_type: {ft_term_type}\")\n",
    "\n",
    "## window size\n",
    "ft_window_size = 5\n",
    "print(f\"ft_window_size: {ft_window_size}\")\n",
    "\n",
    "## dimension = vector size\n",
    "ft_n_dims_factor = 2\n",
    "ft_n_dims = round(n_topics * ft_n_dims_factor)\n",
    "print(f\"ft_n_dims: {ft_n_dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 設定の確認\n",
    "\n",
    "assert use_LDA or use_FastText\n",
    "\n",
    "if use_LDA:\n",
    "    if use_FastText:\n",
    "        encoding_method = \"LDA x FastText\"\n",
    "    else:\n",
    "        encoding_method = \"LDA\"\n",
    "else:\n",
    "    encoding_method = \"FastText\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf933d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tSNE 用\n",
    "\n",
    "## 実行前にサンプリングするかどうか\n",
    "doc_fit_sampling      = True\n",
    "doc_fit_sampling_rate = 0.2\n",
    "\n",
    "## perplexity を決めるパラメター\n",
    "top_perplexity_reduct_rate = 0.5\n",
    "#doc_perplexity_reduct_rate = 0.3 # 段階的に変化させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af87cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UMAP の設定\n",
    "\n",
    "## correlation の他のmetric は良くない\n",
    "umap_metrics     = [ 'correlation',\n",
    "                    'cosine', 'euclidean', 'braycurtis', 'canberra', 'manhattan', 'minkowski'\n",
    "                    'mahalanobis' ]\n",
    "umap_metric      = umap_metrics[0]\n",
    "\n",
    "## n_neighbors\n",
    "umap_n_neighbors = 6\n",
    "\n",
    "## min_dist\n",
    "umap_min_dist    = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c41b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DBSCAN clustering 用\n",
    "\n",
    "DBSCAN_uses_UMAP = True # False ならtSNE を使う\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e582c",
   "metadata": {},
   "source": [
    "# 言語の選別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea941503",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 言語の選別\n",
    "select_languages = True\n",
    "\n",
    "## 言語の割合の均等化\n",
    "balanced = True\n",
    "\n",
    "## 色分けで言語名の変わりに語族を使う\n",
    "color_lang_family = False\n",
    "\n",
    "## 英語をゲルマン語に含める\n",
    "germanic_includes_english = False\n",
    "\n",
    "## スラブ語属を設定する\n",
    "use_slavic = False # russian, czech を一緒にするのは条件付き\n",
    "\n",
    "## ルーマニア語をロマンス語に含める\n",
    "romance_includes_romanian = False\n",
    "\n",
    "## ルーマニア語をスラブ語に含める\n",
    "slavic_includes_romanian = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b731e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##音声解析で使える言語\n",
    "## arabic, chinese, english, esperanto, french, german, icelandic, japanese, korean, spanish, swahili\n",
    "\n",
    "##綴り解析で使える言語\n",
    "## arabic, bengali, chinese, czech, esperanto, english, finnish, french,\n",
    "## galician, german, greek, hebrew, hungarian, icelandic, irish, italian,\n",
    "## japanese, romanian, russian, spanish, swahili, turkish, welsh\n",
    "\n",
    "import re\n",
    "## sound sets\n",
    "sound_set0 = \"arabic, chinese, dutch, english, esperato, french, german, icelandic, \\\n",
    "    japanese, korean, spanish, swahili\"\n",
    "sound_set1 = \"arabic, dutch, english, esperanto, french, german, icelandic, \\\n",
    "    japanese, korean, spanish, swahili\"\n",
    "#\n",
    "sound_set_romance = \"esperanto, french, galician, spanish\"\n",
    "sound_set_germanic = \"english, german, icelandic\"\n",
    "sound_set_semitic = \"arabic, hebrew\"\n",
    "sound_set_asian = \"chinese, japanese, korean\"\n",
    "\n",
    "## spell sets\n",
    "spell_set0 = \"arabic, bengali, chinese, czech, dutch, esperato, english, finnish, french, \\\n",
    "    galician, german, greek, hebrew, hungarian, icelandic, irish, italian, japanese, korean, \\\n",
    "        romanian, russian, spanish, swahili, turkish, welsh\"\n",
    "spell_set1 = \"arabic, czech, hebrew, dutch, english, esperanto, finnish, french, german, greek, \\\n",
    "    hungarian, icelandic, irish, romanian, russian, italian, spanish, swahili, turkish, welsh\"\n",
    "spell_set2 = \"dutch, english, esperanto, french, finnish, german, hungarian, icelandic, irish, italian, \\\n",
    "    spanish, swahili, turkish, welsh\"\n",
    "spell_set3 = \"dutch, english, esperanto, finnish, german, hungarian, irish, swahili, turkish, welsh\"\n",
    "#\n",
    "spell_set_celtic     = \"irish, welsh\"\n",
    "spell_set_celtic_x   = \"irish, welsh, icelandic\"\n",
    "spell_set_germanic   = \"dutch, german, icelandic\"\n",
    "spell_set_germanic_x = \"dutch, english, german, icelandic\"\n",
    "spell_set_romance    = \"esperanto, french, italian, spanish\"\n",
    "spell_set_romance_x  = \"esperanto, french, italian, spanish, romanian\"\n",
    "spell_set_slavic     = \"czech, russian\"\n",
    "\n",
    "## selection\n",
    "selected_langs = re.split(r\",\\s*\", sound_set1)\n",
    "print(f\"selected {len(selected_langs)} languages:\\n{selected_langs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8460a82-a262-4379-8b8f-6aab9f8a262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data to process\n",
    "from pathlib import Path\n",
    "import pprint as pp\n",
    "wd = Path(\".\")\n",
    "dirs = [ x for x in wd.iterdir() if x.is_dir() and not x.match(r\"plot*\") ]\n",
    "if verbose:\n",
    "    print(f\"The following {len(dirs)} directories are potential targets:\")\n",
    "    pp.pprint(dirs)\n",
    "\n",
    "## list up files in target directory \n",
    "target_dir = \"data-words\" # can be changed\n",
    "target_files = sorted(list(wd.glob(f\"{target_dir}/*.csv\")))\n",
    "print(f\"\\n{target_dir} contains {len(target_files)} files to process\")\n",
    "if verbose:\n",
    "    pp.pprint(target_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13883bd6-483a-4480-944b-dc13fcc92f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## データ型の辞書\n",
    "types = re.split(r\",\\s+\", \"spell, sound\")\n",
    "type_settings = { t : 0 for t in types }\n",
    "print(type_settings)\n",
    "\n",
    "## 言語名の辞書\n",
    "lang_settings = { lang : 0 for lang in selected_langs }\n",
    "print(f\"{len(lang_settings.keys())} langs are targeted\")\n",
    "print(lang_settings)\n",
    "\n",
    "## 辞書と統合\n",
    "settings = { **type_settings, **lang_settings }\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b07b80-f580-4e03-ba9c-cd7ad8860c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain data from files\n",
    "check = False\n",
    "setting_keys = list(settings.keys())\n",
    "print(f\"target setting_keys: {setting_keys}\")\n",
    "d_parts = [ ]\n",
    "for lang in lang_settings.keys():\n",
    "    local_settings = settings.copy()\n",
    "    if check:\n",
    "        print(f\"processing: {lang}\")\n",
    "    try:\n",
    "        ## ファイル単位で処理\n",
    "        for f in [ f for f in target_files if lang.capitalize() in str(f) ]:\n",
    "            print(f\"reading: {f}\")\n",
    "            \n",
    "            # 言語名の指定\n",
    "            local_settings[lang] = 1\n",
    "            \n",
    "            # 型名の指定\n",
    "            for key in type_settings.keys():\n",
    "                if key in str(f):\n",
    "                    local_settings[key] = 1\n",
    "                else:\n",
    "                    local_settings[key] = 0 # この変更を見落とさないように\n",
    "            \n",
    "            # ファイル処理\n",
    "            try:\n",
    "                data = pd.read_csv(f, encoding = 'utf-8', sep = \",\", on_bad_lines = 'skip') # Crucially, ...= skip\n",
    "                if check:\n",
    "                    print(data)\n",
    "                #dfx = pd.DataFrame(data, columns = setting_keys)\n",
    "                dfx = pd.DataFrame(data, columns = ['form', 'freq'])\n",
    "                for key in settings.keys():\n",
    "                    dfx[key] = local_settings[key]\n",
    "                    if check:\n",
    "                        print(dfx)\n",
    "                d_parts.append(dfx)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "    except IndexError:\n",
    "        pass\n",
    "#\n",
    "if verbose:\n",
    "    d_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e786fbc1-0de5-4cbe-8ea8-3db6479fab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## データ統合\n",
    "raw_df = pd.concat(d_parts)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7117ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## form の小文字化\n",
    "raw_df[doc_type] = raw_df[doc_type].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd73ea6b-61c6-4b42-9cee-2da4854910d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 言語名= language の列を追加\n",
    "check = False\n",
    "language_vals = [ ]\n",
    "for i, row in raw_df.iterrows():\n",
    "    if check:\n",
    "        print(row)\n",
    "    for j, lang in enumerate(selected_langs):\n",
    "        if check:\n",
    "            print(f\"{i}: {lang}\")\n",
    "        if row[lang] == 1:\n",
    "            language_vals.append(lang)\n",
    "if verbose:\n",
    "    print(language_vals)\n",
    "    len(language_vals)\n",
    "#\n",
    "raw_df['language'] = language_vals\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f55a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 言語族 family 列の追加\n",
    "# germanic\n",
    "if germanic_includes_english:\n",
    "    germanic_langs = [ 'english', 'dutch', 'german', 'icelandic' ]\n",
    "else:\n",
    "    germanic_langs = [ 'dutch', 'german', 'icelandic' ]\n",
    "# celtic\n",
    "celtic_langs       = [ 'irish', 'welsh' ]\n",
    "# romance\n",
    "if romance_includes_romanian:\n",
    "    romance_langs  = [ 'esperanto', 'galician', 'french', 'romanian', 'italian', 'spanish' ]\n",
    "else:\n",
    "    romance_langs  = [ 'esperanto', 'galician', 'french', 'italian', 'spanish' ]\n",
    "# slavic\n",
    "if use_slavic:\n",
    "    if slavic_includes_romanian:\n",
    "        slavic_langs   = [ 'czech', 'romanian', 'russian' ]\n",
    "    else:\n",
    "        slavic_langs   = [ 'czech', 'russian' ]\n",
    "else:\n",
    "    slavic_langs = [ ]\n",
    "#\n",
    "check = False\n",
    "family_vals = [ ]\n",
    "for i, row in raw_df.iterrows():\n",
    "    if check:\n",
    "        print(row)\n",
    "    lang = row['language']\n",
    "    if check:\n",
    "        print(f\"{i}: {lang}\")\n",
    "    # romance langs\n",
    "    if lang in celtic_langs:\n",
    "        family_vals.append('celtic')\n",
    "    # romance langs\n",
    "    elif lang in romance_langs:\n",
    "        family_vals.append('romance')\n",
    "    # germanic langs\n",
    "    elif lang in germanic_langs:\n",
    "        family_vals.append('germanic')\n",
    "    # slavic langs\n",
    "    elif lang in slavic_langs:\n",
    "        family_vals.append('slavic')\n",
    "    # all others\n",
    "    else:\n",
    "        family_vals.append(lang)\n",
    "#\n",
    "if verbose:\n",
    "    print(family_vals)\n",
    "    len(family_vals)\n",
    "#\n",
    "raw_df['family'] = family_vals\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eefbc9-ccf7-40d9-b5c8-d16b48904664",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 文字数の列を追加\n",
    "raw_df['size'] = [ len(x) for x in raw_df[doc_type] ]\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c393cb-6f72-440d-82f6-bea83d78bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 解析データの型指定\n",
    "print(f\"doc_attr: {doc_attr}\")\n",
    "raw_df = raw_df[raw_df[doc_attr] == 1]\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d97b1-be7f-49ce-a660-cb137e1ad8b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 言語の選別\n",
    "if select_languages:\n",
    "    df_new = [ ]\n",
    "    for lang in selected_langs:\n",
    "        df_new.append(raw_df[raw_df[lang] == 1])\n",
    "    raw_df = pd.concat(df_new)\n",
    "#\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded0b91-a72e-4b74-b843-0a2d31f60c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 文字数の分布\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.hist(raw_df['size'], bins = 30)\n",
    "ax.set_xlabel('length of doc')\n",
    "ax.set_ylabel('freq')\n",
    "plt.title(f\"Distribution of document lengths\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88b531",
   "metadata": {},
   "source": [
    "データを長さで濾過"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542fc661-ca98-4127-9698-7ba5d44d00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 長過ぎる語の除外 \n",
    "print(f\"max doc size: {max_doc_size}\")\n",
    "original_size = len(raw_df)\n",
    "raw_df = raw_df[raw_df['size'] <= max_doc_size]\n",
    "max_filtered_size = len(raw_df)\n",
    "print(f\"{original_size - max_filtered_size} cases removed due to max_doc_size\")\n",
    "\n",
    "## 短過ぎる語の除外\n",
    "print(f\"min doc size: {min_doc_size}\")\n",
    "raw_df = raw_df[raw_df['size'] >= min_doc_size]\n",
    "min_filtered_size = len(raw_df)\n",
    "print(f\"{max_filtered_size - min_filtered_size} cases removed due to min_doc_size\")\n",
    "#\n",
    "print(f\"{original_size - min_filtered_size} cases removed in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a07ec-37d9-46a2-9edf-46536888676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 結果の検査 1\n",
    "for type in types:\n",
    "    print(raw_df[type].value_counts(sort = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997f946-e4b7-4bab-bee2-332393a4fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 結果の検査 2\n",
    "for lang in selected_langs:\n",
    "    print(raw_df[lang].value_counts(sort = True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f1a044",
   "metadata": {},
   "source": [
    "英語事例の割合補正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b862bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 統合: 割合補正を適用\n",
    "eng_reduct_factor = 0.2\n",
    "if balanced:\n",
    "    try:\n",
    "        eng_df = raw_df[raw_df['english'] == 1]\n",
    "        non_eng_df = raw_df[raw_df['english'] == 0]\n",
    "        eng_reduced_df = eng_df.sample(round(len(eng_df) * eng_reduct_factor))\n",
    "        raw_df = pd.concat([eng_reduced_df, non_eng_df])\n",
    "    except KeyError:\n",
    "        pass\n",
    "#\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d769a-256d-423d-9347-90bc776fb7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 結果の検査 3\n",
    "for lang in selected_langs:\n",
    "    print(raw_df[lang].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2ea359-e5d4-4c75-a876-408e1601f879",
   "metadata": {},
   "source": [
    "# 解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ede378",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 順序のランダマイズし，基本データを決定\n",
    "import sklearn.utils\n",
    "main_df = sklearn.utils.shuffle(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af58366",
   "metadata": {},
   "source": [
    "DTM 構築の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359cd39-8ccb-40d0-912f-d72c85b6833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ngram の追加\n",
    "\n",
    "## module 探索範囲の拡張\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import re\n",
    "import ngrams\n",
    "import importlib\n",
    "importlib.reload(ngrams)\n",
    "import ngrams_skippy\n",
    "\n",
    "# name the shared variable\n",
    "bases = main_df[doc_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1gram 列の追加\n",
    "unigrams = ngrams.gen_unigrams(bases, sep = r\"\", check = False)\n",
    "unigrams = [ [ u for u in L if len(u) > 0 ] for L in unigrams ]\n",
    "if verbose:\n",
    "    random.sample(unigrams, 5)\n",
    "#\n",
    "main_df['1gram'] = unigrams\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17254df-a360-4cee-9ee4-6f108f1a0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2gram列の追加\n",
    "bigrams = ngrams.gen_bigrams(bases, sep = r\"\", check = False)\n",
    "\n",
    "## 包括的 2gram の作成\n",
    "if ngram_is_inclusive:\n",
    "    bigrams = [ [*b, *u] for b, u in zip(bigrams, unigrams) ]\n",
    "#\n",
    "if verbose:\n",
    "    print(random.sample(bigrams, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d05b78-2b56-4ef3-a2c1-621e6614affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['2gram'] = bigrams\n",
    "if verbose:\n",
    "    main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f386e5-9755-45f9-bdad-bb3e3b77553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3gram列の追加\n",
    "trigrams = ngrams.gen_trigrams(bases, sep = r\"\", check = False)\n",
    "\n",
    "## 包括的 3gram の作成\n",
    "if ngram_is_inclusive:\n",
    "    trigrams = [ [ *t, *b ] for t, b in  zip(trigrams, bigrams) ]\n",
    "#\n",
    "if verbose:\n",
    "    print(random.sample(trigrams, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c012793-7dc4-4463-b7f8-c48aef8ec0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['3gram'] = trigrams\n",
    "if verbose:\n",
    "    main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394cd0ab-e934-4990-b140-4ce3fd697c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## skippy 2grams の生成\n",
    "import ngrams_skippy\n",
    "skippy_2grams = [ ngrams_skippy.gen_skippy_bigrams(x, missing_mark = gap_mark,\n",
    "                                                    max_distance = max_ga_val, check = False)\n",
    "                  for x in main_df['1gram'] ]\n",
    "\n",
    "## 包括的 skippy 2-grams の生成\n",
    "if ngram_is_inclusive:\n",
    "    for i, b2 in enumerate(skippy_2grams):\n",
    "        b2.extend(unigrams[i])\n",
    "#\n",
    "if verbose:\n",
    "    random.sample(skippy_2grams, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de63d20-3381-453a-8e60-fb003c524245",
   "metadata": {},
   "outputs": [],
   "source": [
    "## skippy 2gram 列の追加\n",
    "main_df['skippy2gram'] = skippy_2grams\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf92eed-5d96-4435-a0a0-ae35b4388ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## skippy 3grams の生成\n",
    "import ngrams_skippy\n",
    "skippy_3grams = [ ngrams_skippy.gen_skippy_trigrams(x, missing_mark = gap_mark,\n",
    "                                                         max_distance = max_ga_val, check = False)\n",
    "                  for x in main_df['1gram'] ]\n",
    "\n",
    "## 包括的 skippy 3-grams の生成\n",
    "if ngram_is_inclusive:\n",
    "    for i, t2 in enumerate(skippy_3grams):\n",
    "        t2.extend(skippy_2grams[i])\n",
    "#\n",
    "if verbose:\n",
    "    random.sample(skippy_3grams, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77139cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## skippy 3gram 列の追加\n",
    "main_df['skippy3gram'] = skippy_3grams\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff0b8c",
   "metadata": {},
   "source": [
    "DTM 構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be5a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA 構築の基になる document-term matrix (dtm) を構築\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "bots = main_df[lda_term_type] # 後で参照するので変数化しておく\n",
    "diction = Dictionary(bots)\n",
    "\n",
    "## 結果の確認\n",
    "print(diction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## diction の濾過\n",
    "import copy\n",
    "diction_copy = copy.deepcopy(diction)\n",
    "\n",
    "## filter適用: 実は諸刃の刃で，token数が少ない時には適用しない方が良い\n",
    "print(f\"min freq filter: {term_min_freq}\")\n",
    "print(f\"abuse filter: {term_abuse_threshold}\")\n",
    "\n",
    "apply_filter = True\n",
    "if apply_filter:\n",
    "    diction_copy.filter_extremes(no_below = term_min_freq, no_above = term_abuse_threshold)\n",
    "\n",
    "## check\n",
    "print(diction_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764bbf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Corpus (gensim の用語では corpus) の構築\n",
    "corpus = [ diction.doc2bow(bot) for bot in bots ]\n",
    "\n",
    "## check\n",
    "check = True\n",
    "if check:\n",
    "    sample_n = 5\n",
    "    print(random.sample(corpus, sample_n))\n",
    "#\n",
    "print(f\"Number of documents: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b76aeb",
   "metadata": {},
   "source": [
    "LDA モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532177c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA モデルの構築\n",
    "\n",
    "from gensim.models import LdaModel as LDAmodel\n",
    "print(f\"Building LDA model with n_topics: {n_topics}\")\n",
    "doc_lda = LDAmodel(corpus, id2word = diction, num_topics = n_topics, alpha = 0.01)\n",
    "print(doc_lda) # print(..)しないと中身が見れない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "## LDA のtopic ごとに，関連度の高い term を表示\n",
    "import pandas as pd\n",
    "n_terms = 30 # topic ごとに表示する term 数の指定\n",
    "topic_dfs = [ ]\n",
    "for topic in range(n_topics):\n",
    "    terms = [ ]\n",
    "    for i, prob in doc_lda.get_topic_terms(topic, topn = n_terms):\n",
    "        terms.append(diction.id2token[ int(i) ])\n",
    "    #\n",
    "    topic_dfs.append(pd.DataFrame([terms], index = [ f'topic {topic+1}' ]))\n",
    "#\n",
    "topic_term_df = pd.concat(topic_dfs)\n",
    "\n",
    "## Table で表示\n",
    "topic_term_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4e29a",
   "metadata": {},
   "source": [
    "LDAvis の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e987470",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "## pyLDAvis を使った結果 LDA の可視化: 階層クラスタリングより詳しい\n",
    "import pyLDAvis\n",
    "installed_version = pyLDAvis.__version__\n",
    "print(f\"installed_version: {installed_version}\")\n",
    "\n",
    "if float(installed_version[:3]) > 3.1:\n",
    "    import pyLDAvis.gensim_models as gensimvis\n",
    "else:\n",
    "    import pyLDAvis.gensim as gensimvis\n",
    "#\n",
    "pyLDAvis.enable_notebook()\n",
    "#\n",
    "lda_used     = doc_lda\n",
    "corpus_used  = corpus\n",
    "diction_used = diction\n",
    "\n",
    "## 実行パラメター: MMDS かtSNEを選ぶと \"JSON object is too complex\" error を回避できる \n",
    "LDAvis_use_MMDS  = False\n",
    "LDAvis_use_tSNE  = False\n",
    "if LDAvis_use_MMDS:\n",
    "    vis = gensimvis.prepare(lda_used, corpus_used, diction_used, mds = 'mmds',\n",
    "                            n_jobs = 1, sort_topics = False)\n",
    "elif LDAvis_use_tSNE:\n",
    "    vis = gensimvis.prepare(lda_used, corpus_used, diction_used, mds = 'tsne',\n",
    "                            n_jobs = 1, sort_topics = False)\n",
    "else:\n",
    "    vis = gensimvis.prepare(lda_used, corpus_used, diction_used,\n",
    "                            n_jobs = 1, sort_topics = False)\n",
    "#\n",
    "pyLDAvis.display(vis)\n",
    "## topic を表わす円の重なりが多いならn_topics が多過ぎる可能性がある．\n",
    "## ただし2Dで重なっていても，3Dなら重なっていない可能性もある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e5644-5510-42f5-acf9-295253c3d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA がD に対して生成した topics の弁別性を確認\n",
    "## 得られたtopics を確認\n",
    "topic_dist = doc_lda.get_topics()\n",
    "if verbose:\n",
    "    topic_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0063e1b-c379-419c-b66b-012beb18c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 検査 1: topic ごとに分布の和を取る\n",
    "print(topic_dist.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67262e-e8c3-4391-a82b-987e8864c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 検査 2: 総和を求める: n_topics にほぼ等しいなら正常\n",
    "print(topic_dist.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## term エンコード値の分布を確認\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (4,4))\n",
    "df_size = len(topic_dist)\n",
    "sampling_rate = 0.5\n",
    "sample_n = round(df_size * sampling_rate)\n",
    "topic_sampled = random.sample(list(topic_dist), sample_n)\n",
    "\n",
    "T = sorted([ sorted(x, reverse = True) for x in topic_sampled ])\n",
    "plt.plot(T, range(len(T)))\n",
    "plt.title(f\"Distribution of sorted values ({sample_n} samples) for term encoding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038ce17-cd0d-47fd-912c-08539c68d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tSNE を使った topics のグループ化 (3D)\n",
    "\n",
    "from sklearn.manifold import TSNE as tSNE\n",
    "import numpy as np\n",
    "\n",
    "## tSNE のパラメターを設定\n",
    "## n_components は射影先の空間の次元: n_components = 3 なら3次元空間に射影\n",
    "ppl_val = round(len(topic_dist) * top_perplexity_reduct_rate)\n",
    "top_tSNE_3d = tSNE(n_components = 3, random_state = 0, perplexity = ppl_val, n_iter = 1000)\n",
    "\n",
    "## データに適用\n",
    "top_tSNE_3d_fitted = top_tSNE_3d.fit_transform(np.array(topic_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e28cc3-34af-4760-bd55-61fa4ea0f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotlyを使って tSNE の結果の可視化 (3D)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "top_fitted = top_tSNE_3d_fitted\n",
    "fig = go.Figure(data = [go.Scatter3d(\n",
    "        x = top_fitted[:,0], y = top_fitted[:,1], z = top_fitted[:,2],\n",
    "                                     mode = 'markers')\n",
    "                                     ])\n",
    "\n",
    "## 3D 散布図にラベルを追加する処理は未実装\n",
    "title_val = f\"t-SNE 3D map of LDA topics (#topics: {n_topics}; doc: {doc_attr}; term: {lda_term_type})\"\n",
    "fig.update_layout(autosize = False, width = 600, height = 600, title = title_val)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5502e",
   "metadata": {},
   "source": [
    "# doc 分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b32a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed78ca-5941-4361-8754-2765916bac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA モデルを使って doc をエンコード\n",
    "\n",
    "print(f\"get LDA encodings for documents\")\n",
    "check    = False\n",
    "lda_encoding = [ ]\n",
    "for i, row in main_df.iterrows():\n",
    "    if check:\n",
    "        print(f\"row: {row}\")\n",
    "    doc = row[doc_type]\n",
    "    bot = row[lda_term_type]\n",
    "    ## get_document_topics(..) では　minimu_probability = 0 としないと\n",
    "    ## 値が十分に大きな topics に関してだけ値が取れる\n",
    "    enc = doc_lda.get_document_topics(diction.doc2bow(bot), minimum_probability = 0)\n",
    "    if check:\n",
    "        print(f\"enc: {enc}\")\n",
    "    lda_encoding.append([ e[-1] for e in enc ])\n",
    "#\n",
    "print(random.sample(lda_encoding, 2))\n",
    "print(len(lda_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastText を使ったエンコードの準備\n",
    "from gensim.models import FastText\n",
    "\n",
    "## build a model: for better comparison, vector_size should be equal to n_topics\n",
    "print(f\"get FastText encodings for documents\")\n",
    "## emulate sentences by concatenating character 1-grams by whitespaces\n",
    "ft_corpus = [ \" \".join(x) for x in main_df[ft_term_type] ]\n",
    "random.sample(ft_corpus, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastText を使ったエンコード\n",
    "ft_model = FastText(ft_corpus,\n",
    "                    vector_size = ft_n_dims,\n",
    "                    window = ft_window_size,\n",
    "                    min_count = 1, sg = 1)\n",
    "## ft_model は dict\n",
    "ft_encoding = [ ft_model.wv[doc] for doc in ft_corpus ]\n",
    "print(random.sample(ft_encoding, 1))\n",
    "print(f\"data size: {len(ft_encoding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ffebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doc_encoding の選択\n",
    "\n",
    "if use_FastText:\n",
    "    if use_LDA: ## LDA x FastText\n",
    "        doc_encoding = [ np.concatenate([x, y], dtype = object) for x, y in zip(lda_encoding, ft_encoding) ]\n",
    "    else: ## FastText only\n",
    "        doc_encoding = ft_encoding\n",
    "else: ## LDA only\n",
    "    doc_encoding = lda_encoding\n",
    "#\n",
    "print(random.sample(doc_encoding, 1))\n",
    "print(f\"data size: {len(doc_encoding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd15fd3-e1e0-4a83-a44e-69f87b6b6521",
   "metadata": {},
   "outputs": [],
   "source": [
    "## df にenc 列の追加\n",
    "#df['enc'] = [ list(map(lambda x: x[1], y)) for y in doc_encoding ]\n",
    "main_df['enc'] = doc_encoding\n",
    "main_df['enc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23296f56-c71a-46d8-9fb6-c351ca162661",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 濾過前のエンコーディングのstd の分布を確認\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (6,4))\n",
    "plt.hist([ np.std(x) for x in main_df['enc'] ], bins = 20)\n",
    "plt.title(f\"Distribution of std values among {encoding_method} encodings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44386545-b8d6-4e41-b4d0-3d5d090f1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 一様分布の事例を除外\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "check = False\n",
    "doc_enc = main_df['enc']\n",
    "max_std = max([ np.std(x) for x in doc_enc])\n",
    "if check:\n",
    "    print(f\"std max: {max_std}\")\n",
    "\n",
    "min_std = min([ np.std(x) for x in doc_enc])\n",
    "if check:\n",
    "    print(f\"std min: {min_std}\")\n",
    "\n",
    "first_min_std = list(sorted(set([ np.std(x) for x in doc_enc])))[-0]\n",
    "print(f\"std 1st min: {first_min_std}\")\n",
    "\n",
    "second_min_std = list(sorted(set([ np.std(x) for x in doc_enc])))[-1]\n",
    "print(f\"std 2nd min: {second_min_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb41541-9309-40ee-9a3d-1ec361073255",
   "metadata": {},
   "outputs": [],
   "source": [
    "## df_filtered の定義\n",
    "import numpy as np\n",
    "\n",
    "print(f\"{len(main_df)} instances before filtering\")\n",
    "\n",
    "## 閾値は2番目に小さい値より小さく最小値よりは大きな値であるべき\n",
    "std_threshold = second_min_std / 6 # 穏健な値を得るために 6で割った\n",
    "print(f\"std_threshold: {std_threshold}\")\n",
    "\n",
    "## Rっぽい次のコードは通らない\n",
    "#df_filtered = df[ df['encoding'] > std_threshold ]\n",
    "## 通るのは次のコード: Creating a list of True/False and apply it to DataFrame \n",
    "std_tested = [ False if np.std(x) < std_threshold else True for x in main_df['enc'] ]\n",
    "df_filtered = main_df[ std_tested ]\n",
    "print(f\"Filtering leaves {len(df_filtered)} instances: ({len(main_df) - len(df_filtered)} instances removed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed27163",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doc エンコード値の分布を確認\n",
    "\n",
    "sample_n = 50\n",
    "E = sorted([ sorted(x, reverse = True) for x in df_filtered['enc'].sample(sample_n) ])\n",
    "plt.figure(figsize = (4,4))\n",
    "plt.plot(E, range(len(E)))\n",
    "plt.title(f\"Distribution of sorted {encoding_method} encodings for sampled {sample_n} docs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949740c-abad-4656-b925-77c09ecb3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['language'].value_counts(sort = True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37dcc1c",
   "metadata": {},
   "source": [
    "Dim Reduct 用の事例サンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fafdcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Dim Reduct 用の事例サンプリング = doc_fit_df の定義\n",
    "if doc_fit_sampling:\n",
    "    doc_fit_df_original = df_filtered.copy()\n",
    "    sample_n = round(len(doc_fit_df_original) * doc_fit_sampling_rate)\n",
    "    doc_fit_df = doc_fit_df_original.sample(sample_n)\n",
    "    print(f\"doc_fit_df has {len(doc_fit_df)} rows after sampling\")\n",
    "else:\n",
    "    doc_fit_df = df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8d14e-d6a2-4482-9c0f-d922cd6a611d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_fit_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee37ce-75c1-4419-96a8-9db9a633ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_fit_df['language'].value_counts(sort = True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfde434",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_fit_df['family'].value_counts(sort = True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15b1d7",
   "metadata": {},
   "source": [
    "UMAP を使った可視化 (3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 必要に応じて\n",
    "#!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UMAP を使った documents のグループ化\n",
    "\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "\n",
    "## UMAP の生成\n",
    "UMAP_3d = umap.UMAP(n_components = 3, random_state = 1, n_jobs = 1,\n",
    "                    metric = umap_metric,\n",
    "                    n_neighbors = umap_n_neighbors, min_dist = umap_min_dist)\n",
    "\n",
    "## データに適用\n",
    "doc_enc = np.array(list(doc_fit_df['enc']))\n",
    "doc_UMAP_3d = UMAP_3d.fit_transform(doc_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a09c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "## Plotlyを使って UMAP の結果の可視化 (3D)\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "## 色分けの単位を選択\n",
    "if color_lang_family:\n",
    "    colored_var = 'family'\n",
    "else:\n",
    "    colored_var = 'language'\n",
    "## umap_df を作成\n",
    "X = zip(doc_UMAP_3d[:,0], doc_UMAP_3d[:,1], doc_UMAP_3d[:,2],\n",
    "        doc_fit_df[colored_var], doc_fit_df[doc_type]) # zip(..)が必要\n",
    "umap_df = pd.DataFrame(X, columns = ['D1', 'D2', 'D3', colored_var, doc_type])\n",
    "#\n",
    "fig = go.Figure()\n",
    "for lang_name in sorted(set(umap_df[colored_var])):\n",
    "    # set marker size of the target domain\n",
    "    traced = umap_df[ umap_df['language'] == lang_name ]\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x = traced['D1'], y = traced['D2'], z = traced['D3'],\n",
    "            name = lang_name.title(),\n",
    "            mode = 'markers',\n",
    "            marker = dict(size = 4, opacity = 0.8),\n",
    "            marker_colorscale = color_palette, # worked??\n",
    "            showlegend = True\n",
    "        )\n",
    "    )\n",
    "\n",
    "## 3D 散布図にラベルを追加する処理は未実装\n",
    "df_size = len(umap_df)\n",
    "## title_header\n",
    "title_header = f\"UMAP 3D (metric: {umap_metric}; {umap_n_neighbors} neighbors; min dist: {umap_min_dist}) of {df_size} encodings via\\n\"\n",
    "## title_body\n",
    "if use_FastText:\n",
    "    if use_LDA:\n",
    "        title_body = f\"LDA ({n_topics} topics; term:0 {lda_term_type}) x FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "    else:\n",
    "        title_body = f\"FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "else:\n",
    "    title_body = f\"LDA ({n_topics} topics; term: {lda_term_type})\"\n",
    "#\n",
    "title_val = title_header + title_body\n",
    "fig.update_layout(title = dict(text = title_val, font_size = 12), autosize = False, width = 700, height = 700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3be155",
   "metadata": {},
   "source": [
    "t-SNE の結果の可視化 (3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0401350-7dc5-4ee8-867b-2b60e0245a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## tSNE の結果の可視化: Plotly を使った 3D 描画\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE as tSNE\n",
    "import plotly.express as pex\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## tSNE のパラメターを設定\n",
    "max_perplexity_factor = 3\n",
    "perplexity_max_val = round(len(doc_fit_df)/max_perplexity_factor)\n",
    "print(f\"perplexity_max_val: {perplexity_max_val}\")\n",
    "\n",
    "divider = 5\n",
    "perplexity_increment = round(perplexity_max_val/divider)\n",
    "print(f\"perplexity_increment: {perplexity_increment}\")\n",
    "\n",
    "## tSNE を段階的に実行\n",
    "df_size = len(doc_fit_df)\n",
    "exit_after_step = 3 # 15 is largest enough to complete the all steps\n",
    "ppl_vals = enumerate(range(5, perplexity_max_val, perplexity_increment))\n",
    "for step, ppl_val in ppl_vals:\n",
    "    ## 早期終了の判定\n",
    "    if step >= exit_after_step:\n",
    "        continue\n",
    "    ## tSNE 事例の生成\n",
    "    tSNE_3d_varied = tSNE(n_components = 3, random_state = 0, perplexity = ppl_val,\n",
    "                          n_iter = 1000)\n",
    "\n",
    "    ## データに適用\n",
    "    doc_enc = np.array(list(doc_fit_df['enc']))\n",
    "    doc_tSNE_3d_varied = tSNE_3d_varied.fit_transform(doc_enc)\n",
    "\n",
    "    ## 色分けの単位を選択\n",
    "    if color_lang_family:\n",
    "        colored_var = 'family'\n",
    "    else:\n",
    "        colored_var = 'language'\n",
    "    ##\n",
    "    X = zip(doc_tSNE_3d_varied[:,0], doc_tSNE_3d_varied[:,1], doc_tSNE_3d_varied[:,2],\n",
    "            doc_fit_df[colored_var]) # zip(..)が必要\n",
    "    dfx = pd.DataFrame(X, columns = ['D1', 'D2', 'D3', colored_var])\n",
    "    ## 作図\n",
    "    fig = go.Figure()\n",
    "    for lang in np.unique(main_df[colored_var]):\n",
    "        traced = dfx[dfx[colored_var] == lang]\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x = traced['D1'], y = traced['D2'], z = traced['D3'],\n",
    "                name = lang.title(), mode = 'markers', marker = dict(size = 3),\n",
    "                marker_colorscale = color_palette, # worked??\n",
    "                showlegend = True\n",
    "            )\n",
    "        )\n",
    "    ## 題の指定\n",
    "    title_header = f\"t-SNE 3D map (ppl: {ppl_val}) of {df_size} encodings for <{doc_attr}> via\\n\"\n",
    "    if use_FastText:\n",
    "        if use_LDA:\n",
    "            title_body = f\"LDA ({n_topics} topics; term: {lda_term_type}) x FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "        else:\n",
    "            title_body = f\"FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "    else:\n",
    "        title_body = f\"LDA ({n_topics} topics; term: {lda_term_type})\"\n",
    "    title_val = title_header + title_body\n",
    "    fig.update_layout(title = dict(text = title_val, font_size = 13),\n",
    "                      autosize = False, width = 600, height = 600)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca06974",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 必要に応じて adjustText を道入\n",
    "#!pip install -U adjustText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ccc5be",
   "metadata": {},
   "source": [
    "t-SNE の結果の可視化 (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001749fd-e7aa-4198-a613-e0d9a983bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tSNE の結果の可視化 (2D)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE as tSNE\n",
    "import plotly\n",
    "import plotly.express as pex\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "## 次の設定は arabic が文字化けする\n",
    "#plt.rcParams[\"font.family\"] = \"Hiragino Sans\" # Windows は別のフォント名を指定する必要がある\n",
    "\n",
    "## tSNE 事例の生成\n",
    "ppl_divider = 7\n",
    "perplexity_selected = round(len(doc_fit_df)/ppl_divider)\n",
    "doc_perplexity_val = perplexity_selected\n",
    "doc_tSNE_3d = tSNE(n_components = 3, random_state = 0, perplexity = perplexity_selected, n_iter = 1000)\n",
    "\n",
    "## 色分けの単位を選択\n",
    "if color_lang_family:\n",
    "    colored_var = 'family' # 一部の言語を celtic, germanic, romance, slavic 属に統合\n",
    "else:\n",
    "    colored_var = 'language' # 言語名そのまま\n",
    "\n",
    "## データに適用\n",
    "doc_enc = np.array(list(doc_fit_df['enc']))\n",
    "doc_tSNE_3d = doc_tSNE_3d.fit_transform(doc_enc)\n",
    "\n",
    "## plot_df の定義\n",
    "X = zip(doc_tSNE_3d[:,0], doc_tSNE_3d[:,1], doc_tSNE_3d[:,2], doc_fit_df[colored_var]) # zip(..)が必要\n",
    "plot_df = pd.DataFrame(X, columns = ['D1', 'D2', 'D3', colored_var])\n",
    "\n",
    "## 描画\n",
    "plt.figure(figsize = (5, 5))\n",
    "## 配色\n",
    "plt.set_colors = pex.colors.qualitative.Plotly\n",
    "## labeling\n",
    "lab_sampling_rate = 0.03\n",
    "lab_sample_n = round(len(doc_fit_df) * lab_sampling_rate)\n",
    "sampled_keys = [ doc[:max_doc_size] for doc in random.sample(list(doc_fit_df[doc_type]), lab_sample_n) ]    \n",
    "##\n",
    "check = False\n",
    "early_exit = False\n",
    "for step, roll in enumerate([ np.roll([0,1,2], -i) for i in range(0,3) ]):\n",
    "    ## 早期停止の判定\n",
    "    if early_exit:\n",
    "        if step > 0:\n",
    "            continue\n",
    "    ## 通常処理\n",
    "    if check:\n",
    "        print(roll)\n",
    "    X, Y = plot_df.iloc[:, roll[0]], plot_df.iloc[:, roll[1]]\n",
    "    gmax = max(X.max(), Y.max())\n",
    "    gmin = min(X.min(), Y.min())\n",
    "    plt.xlim(gmin, gmax)\n",
    "    plt.ylim(gmin, gmax)\n",
    "\n",
    "    ## 配色\n",
    "    colormap = color_palette # == pex.colors.qualitative.Light24\n",
    "    lang_list = list(set(doc_fit_df[colored_var]))\n",
    "    cmapped = [ colormap[lang_list.index(lang)] for lang in plot_df[colored_var] ]\n",
    "    \n",
    "    ## 作図\n",
    "    scatter = plt.scatter(X, Y, s = 30, c = cmapped, edgecolors = 'w')\n",
    "\n",
    "    ## 文字を表示する事例のサンプリング\n",
    "    texts = [ ]\n",
    "    ## labels の生成\n",
    "    for x, y, s in zip(X, Y, sampled_keys):\n",
    "        texts.append(plt.text(x, y, s, size = 7, color = 'blue'))\n",
    "        ## label に repel を追加: adjustText package の導入が必要\n",
    "    adjust_text(texts,\n",
    "        #force_points = 0.2,\n",
    "        ## Comment out the following line if you get AttributionError\n",
    "        force_text = (.1, .2), expand_points = (1, 1), expand_text = (1, 1),\n",
    "        arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "    #\n",
    "    title_header = f\"t-SNE (ppl: {perplexity_selected}) 2D map of {len(doc_fit_df)} encodings for <{doc_attr}>\\n\"\n",
    "    title_body = f\"of {', '.join([ l.title() for l in sorted([ l[:3] for l in selected_langs])])}\\n\"\n",
    "    title_tail = f\"via LDA ({n_topics} topics; terk: {lda_term_type})\"          \n",
    "    plt.title(title_header + title_body + title_tail)\n",
    "    #plt.legend(df['language'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4034c7",
   "metadata": {},
   "source": [
    "DBSCAN を使ったクラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DBSCAN を使ったクラスタリング\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "DBSCAN_uses_UMAP = True\n",
    "if DBSCAN_uses_UMAP:\n",
    "    dbscan_source = doc_UMAP_3d\n",
    "else:\n",
    "    dbscan_source = doc_tSNE_3d\n",
    "\n",
    "## eps は事例ごとに調節が必要\n",
    "min_samples_val = 2\n",
    "## looking for optimal eps val compatible with color palette\n",
    "max_n_clusters = 24 # This depends on the differetiation in color palette used\n",
    "dbscan_clustered = None\n",
    "cluster_ids = None\n",
    "print(f\"Looking for the optimal eps val...\")\n",
    "max_val = 5 # needs to be effectively large\n",
    "eps_vals = np.arange(max_val, 0.005, -0.05)\n",
    "check = False\n",
    "for eps_val in eps_vals:\n",
    "    if check:\n",
    "        print(f\"testing eps = {eps_val:0.4f}\")\n",
    "    dbscan_clustered_local = DBSCAN(eps = eps_val, min_samples = min_samples_val).fit(dbscan_source)\n",
    "    cluster_ids_local = dbscan_clustered_local.labels_\n",
    "    if check:\n",
    "        print(f\"result: {np.unique(cluster_ids_local)}\")\n",
    "    try:\n",
    "        assert len(np.unique(cluster_ids_local)) <= max_n_clusters\n",
    "        dbscan_clustered = dbscan_clustered_local\n",
    "        cluster_ids = cluster_ids_local\n",
    "    except AssertionError:\n",
    "        break\n",
    "#\n",
    "print(f\"final result: {np.unique(cluster_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fbadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## 日本語表示のための設定\n",
    "#plt.rcParams[\"font.family\"] = \"Hiragino sans\" # Windows/Linux の場合は別のフォントを指定\n",
    "\n",
    "## 凡例の文字の大きさを指定\n",
    "param_vals = {'legend.fontsize': 7, 'legend.handlelength': 2}\n",
    "plt.rcParams.update(param_vals)\n",
    "\n",
    "## 描画\n",
    "fig = plt.figure(figsize = (6, 6))\n",
    "#\n",
    "d1, d2 = dbscan_source[:,0], dbscan_source[:,1]\n",
    "sns.scatterplot(x = d1, y = d2, hue = [ f\"cluster {l}\" for l in dbscan_clustered.labels_ ]) # requires Searborn\n",
    "\n",
    "## 文字を表示する事例のサンプリング\n",
    "relative = True\n",
    "if relative:\n",
    "    lab_sampling_rate = 0.04 # サンプリング率の指定\n",
    "    lab_sample_n = round(lab_sampling_rate * len(plot_df))\n",
    "else:\n",
    "    lab_sample_n = 30 # 絶対数の指定\n",
    "\n",
    "## 事例名の生成\n",
    "texts = [ ]\n",
    "sampled_keys = [ x[:max_doc_size] for x in random.sample(list(doc_fit_df[doc_type]), lab_sample_n) ]\n",
    "for x, y, s in zip(d1, d2, sampled_keys):\n",
    "    texts.append(plt.text(x, y, s, size = 7, color = 'blue'))\n",
    "\n",
    "## label に repel を追加: adjustText package の導入が必要\n",
    "adjust_text(texts,\n",
    "        #force_points = 0.2,\n",
    "        ## Comment out the following line if you get AttributionError\n",
    "        force_text = (.1, .2), expand_points = (1, 1), expand_text = (1, 1),\n",
    "        arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "    \n",
    "## 題名を指定\n",
    "## used_domains\n",
    "used_lang_list = f\"{', '.join([ l.title() for l in [ l[:3] for l in selected_langs] ])}\"\n",
    "## title_header\n",
    "df_size = len(plot_df)\n",
    "if DBSCAN_uses_UMAP:\n",
    "    title_header = f\"2D view of UMAP (metric: {umap_metric}, n_neighbors: {umap_n_neighbors}; min_dist: {umap_min_dist}) for {df_size} encodings for <{doc_attr}> \\nin {used_lang_list} via\\n\"\n",
    "else:\n",
    "    title_header = f\"2D view of t-SNE (ppl: {doc_perplexity_val}) for {df_size} encodings for <{doc_attr}> in\\n{used_lang_list} via\\n\"\n",
    "## title_val\n",
    "if use_FastText:\n",
    "    if use_LDA:\n",
    "        title_body = f\"LDA ({n_topics} topics; term: {lda_term_type}) x FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "    else:\n",
    "        title_boday = f\"FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "else:\n",
    "    title_val = f\"LDA ({n_topics} topics; term: {lda_term_type})\"\n",
    "## clustering_method\n",
    "clustering_method = f\"\\nclustered by DBSCAN (eps: {eps_val:0.3f}; min_samples: {min_samples_val})\"\n",
    "#\n",
    "title_val = title_header + title_body + clustering_method\n",
    "plt.title(title_val)\n",
    "plt.show()\n",
    "## 局在の程度は character 1-gram, (skippy) 2-gram, (skippy) 3-gram のどれを使うかで違って来る．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "## co-clusteredness に基づく言語間類似度の計算\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#DBSCAN_uses_UMAP = True # tSNE は有効でないようだ\n",
    "if DBSCAN_uses_UMAP:\n",
    "    dbscan_source = doc_UMAP_3d\n",
    "else:\n",
    "    dbscan_source = doc_tSNE_3d\n",
    "\n",
    "## eps, min_samples は事例ごとに調節が必要\n",
    "min_samples_val = 2\n",
    "\n",
    "## 精度を上げるために，クラスター数を多目にする\n",
    "scaling_factor = 3\n",
    "max_n_clusters_for_correl = round(len(selected_langs) * scaling_factor)\n",
    "print(f\"max_n_clusters_for_correl\")\n",
    "\n",
    "print(f\"Looking for the optimal eps val...\")\n",
    "check = False\n",
    "x_dbscan_clustered = None\n",
    "x_cluster_ids = None\n",
    "max_val = 5 # needs to be effectively large\n",
    "eps_vals = np.arange(max_val, 0.005, -0.05)\n",
    "for eps_val in eps_vals:\n",
    "    if check:\n",
    "        print(f\"testing eps = {eps_val:0.4f}\")\n",
    "    x_dbscan_clustered_local = DBSCAN(eps = eps_val, min_samples = min_samples_val).fit(dbscan_source)\n",
    "    x_cluster_ids_local = x_dbscan_clustered_local.labels_\n",
    "    if check:\n",
    "        print(f\"result: {np.unique(x_cluster_ids_local)}\")\n",
    "    try:\n",
    "        assert len(np.unique(x_cluster_ids_local)) <= max_n_clusters_for_correl\n",
    "        x_dbscan_clustered = x_dbscan_clustered_local\n",
    "        x_cluster_ids = x_cluster_ids_local\n",
    "    except AssertionError:\n",
    "        break\n",
    "#\n",
    "print(f\"final result: {np.unique(x_cluster_ids)}\")\n",
    "\n",
    "## clusterごとに言語の帰属数を集計 \n",
    "bindings = zip(doc_fit_df[doc_type], doc_fit_df['language'], x_dbscan_clustered.labels_)\n",
    "binding_df = pd.DataFrame(bindings, columns = ['form', 'language', 'cluster'])\n",
    "\n",
    "lang_names = sorted(set(binding_df['language']))\n",
    "print(f\"lang_names: {lang_names}\")\n",
    "\n",
    "cluster_ids = sorted(set(binding_df['cluster']))\n",
    "print(f\"cluster ids: {cluster_ids}\")\n",
    "\n",
    "clusterwise_counts = { lang_name : None for lang_name in lang_names }\n",
    "for lang_name in lang_names:\n",
    "    counts = [ ]\n",
    "    selected = binding_df[ binding_df['language'] == lang_name ]\n",
    "    for i, cluster_id in enumerate(cluster_ids):\n",
    "        matched = selected[ selected['cluster'] == cluster_id ]\n",
    "        n_matches = len(matched)\n",
    "        if n_matches == 0:\n",
    "            counts.append(0)\n",
    "        else:\n",
    "            counts.append(int(n_matches))\n",
    "    clusterwise_counts[lang_name] = counts\n",
    "\n",
    "## クラスターでの生起個数の間の相関を計算\n",
    "clusterwise_counts_df = pd.DataFrame.from_dict(clusterwise_counts) ## Truly versatile\n",
    "clusterwise_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Heatmap で可視化\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "counts_df_normalized = (clusterwise_counts_df - clusterwise_counts_df.min())/(clusterwise_counts_df.max() - clusterwise_counts_df.min())\n",
    "corr_df = counts_df_normalized.corr()\n",
    "#corr_df.sort_index(axis = 0, inplace = True)\n",
    "#corr_df.sort_index(axis = 1, inplace = True)\n",
    "\n",
    "n_langs = len(selected_langs)\n",
    "fig = plt.figure(figsize = (round(n_langs * 0.65), 7))\n",
    "sns.heatmap(corr_df, cmap = sns.color_palette('coolwarm', 10),\n",
    "            annot = True, fmt = '.2f', vmin = -1, vmax = 1)\n",
    "\n",
    "title_val = f\"Heatmap of correlations among co-clusteredness over {len(cluster_ids)} clusters with {len(dbscan_source)} {doc_attr}s from\\n{n_langs} languages (LDA n_top: {n_topics}; term_type: {lda_term_type})\"\n",
    "plt.title(title_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e0ef9",
   "metadata": {},
   "source": [
    "階層クラスタリングのための部分サンプリングの追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ed9b8-0e8c-4b73-9f5c-c5932bcfd6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 階層クラスタリングのための事例のサンプリング\n",
    "\n",
    "df_size = len(doc_fit_df)\n",
    "hc_sampling_rate = 0.1 # 大きくし過ぎると図が見にくい\n",
    "hc_sample_n = round(df_size * hc_sampling_rate)\n",
    "doc_hc_df = doc_fit_df.sample(hc_sample_n)\n",
    "##\n",
    "print(f\"{hc_sample_n} rows are sampled\")\n",
    "doc_hc_df[colored_var].value_counts(sort = True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea6e05",
   "metadata": {},
   "source": [
    "階層クラスタリングの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doc 階層クラスタリングの実行\n",
    "\n",
    "import numpy as np\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt\n",
    "## 次の設定は arabic, bengali が文字化けする\n",
    "#plt.rcParams[\"font.family\"] = \"Hiragino Sans\" # Windows は別のフォント名を指定する必要がある\n",
    "\n",
    "## 描画サイズの指定\n",
    "plt.figure(figsize = (5, round(len(doc_hc_df) * 0.17))) # This needs to be run here, before dendrogram construction.\n",
    "\n",
    "## 事例ラベルの生成\n",
    "label_vals = [ x[:max_doc_size] for x in list(doc_hc_df[doc_type]) ] # truncate doc keys\n",
    "\n",
    "## 樹状分岐図の作成\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "## 距離行列の生成\n",
    "doc_hc_enc = list(doc_hc_df['enc'])\n",
    "doc_linkage = linkage(doc_hc_enc, method = 'ward', metric = 'euclidean')\n",
    "dendrogram(doc_linkage, orientation = 'left', labels = label_vals, leaf_font_size = 8)\n",
    "\n",
    "## 描画\n",
    "plt.title(f\"Hierarchical clustering of (sampled) {len(doc_hc_df)} (= {100 * hc_sampling_rate}%) <{doc_attr}>s of\\n \\\n",
    "    {', '.join([ l.title()[:3] for l in sorted(selected_langs)])}\\n \\\n",
    "    encoded via LDA ({n_topics} topics); term: {lda_term_type})\")\n",
    "\n",
    "## ラベルに language に対応する色を付ける\n",
    "lang_colors = { lang_name : i for i, lang_name in enumerate(np.unique(doc_hc_df[colored_var])) }\n",
    "ax = plt.gca()\n",
    "for ticker in ax.get_ymajorticklabels():\n",
    "    form = ticker.get_text()\n",
    "    row = doc_hc_df.loc[doc_hc_df[doc_type] == form]\n",
    "    lang = row[colored_var].to_string().split()[-1] # trick\n",
    "    try:\n",
    "        lang_id = lang_colors[lang]\n",
    "    except (TypeError, KeyError):\n",
    "        print(f\"color encoding error at: {lang}\")\n",
    "    #\n",
    "    ticker.set_color(color_palette[lang_id])\n",
    "#\n",
    "plt.show()\n",
    "## A few Turkish words will be partially garbled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
